m <- as.matrix(dtm)
m
filePath <- "~/Dropbox/R/TidyTextMining/1Peter"
text <- readLines(filePath)
tokenize_sentences(text)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
m
dtm
dtm$ncol
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
docs
m <- as.matrix(dtm)
m
m <- as.matrix(docs)
View(docs)
View(docs)
docs %>% unnest_tokens(word, text)
library(dplyr)
library(janeaustenr)
library(tidytext)
book_words <- austen_books()%>%
unnest_tokens(word, text) %>%
count(book, word, sort = TRUE) %>%
ungroup()
total_words <- book_words %>% group_by(book) %>% summarize(total = sum(n))
book_words <- left_join(book_words, total_words)
book_words
book_words <- austen_books()%>%
unnest_tokens(word, text) %>%
count(book, word, sort = TRUE) %>%
ungroup()
book_wrods
book_words
book_words <- left_join(book_words, total_words)
book_words
total_books
total_words
ggplot(book_words, aes(n/total, fill = book )) +
geom_histogram(show.legend =FALSE) +
xlim(0.0009) +
facet_wrap(~book, ncol = 2, scale = "free_y")
library(ggplot2)
ggplot(book_words, aes(n/total, fill = book )) +
geom_histogram(show.legend =FALSE) +
xlim(0.0009) +
facet_wrap(~book, ncol = 2, scale = "free_y")
ggplot(book_words, aes(n/total, fill = book )) +
geom_histogram(show.legend =FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~book, ncol = 2, scales = "free_y")
bind_tf_idf(word, book, n)
book_words <- book_words %>%
bind_tf_idf(word, book, n)
book_words
book_words #>#
select(-total) $>$
arrange(desc(tf_idf))
book_words #>#
select(-total) $>$
arrange(desc(tf_idf))
book_words #>#
select(-total) #>#
arrange(desc(tf_idf))
book_words %>%
select(-total) %>%
arrange(desc(tf_idf))
book_words %>%
filter(book == "Pride & Prejudice") %>%
select(-total) %>%
arrange(desc(tf_idf))
library(dplyr)
library(gutenbergr)
titles <- c("Twenty Thousand Leagues under the Sea", "The War of the Worlds",
"Pride and Prejudice", "Great Expectations")
books <- gutenberg_works(title %in% titles) %>%
gutenberg_download(meta_fileds = "title")
books <- gutenberg_works(title %in% titles) %>%
gutenberg_download(meta_fields = "title")
books
View(books)
by_chapter <- books %>% mutate(chapter = cumsum(str_detect(text, regex("^chapter", ignore.case = TRUE)))) %>%
ungroup() %>%
filter(chapter > 0)
by_chapter_word <- by_chapter %>%
unite(title_chapter, title, chapter %>%
unnest_tokens(word,text)
by_chapter_word <- by_chapter %>%
unite(title_chapter, title, chapter) %>%
unnest_tokens(word,text)
by_chapter_word <- by_chapter %>%
unite(title_chapter, title, chapter) %>%
unnest_tokens(word,text)
library(tidytext)
library(stringr)
library(tidyr)
by_chapter <- books %>% mutate(chapter = cumsum(str_detect(text, regex("^chapter", ignore.case = TRUE)))) %>%
ungroup() %>%
filter(chapter > 0)
by_chapter_word <- by_chapter %>%
unite(title_chapter, title, chapter) %>%
unnest_tokens(word,text)
word_counts <- by_chapter_word %>%
anti_join(stop_words) %>%
ungroup()
word_counts
word_counts <- by_chapter_word %>%
anti_join(stop_words) %>%
count(title_chapter, word, sort = TRUE) %>%
ungroup()
word_counts
chapter_dtm <- word_counts %>%
cast_dtm(title_chapter, word, n)
chapters_dtm
chapter_dtm
titles <- c("Twenty Thousand Leagues under the Sea", "The War of the Worlds",
"Pride and Prejudice", "Great Expectations")
# Download books from gutenberg
books <- gutenberg_works(title %in% titles) %>%
gutenberg_download(meta_fields = "title")
books
by_chapter <- books %>% mutate(chapter = cumsum(str_detect(text, regex("^chapter", ignore.case = TRUE)))) %>%
ungroup() %>%
filter(chapter > 0)
by_chapter_word <- by_chapter %>%
unite(title_chapter, title, chapter) %>%
unnest_tokens(word,text)
word_counts <- by_chapter_word %>%
anti_join(stop_words) %>%
count(title_chapter, word, sort = TRUE) %>%
ungroup()
word_counts
source('~/Dropbox/R/TidyTextMining/InverseTermFrequency.R')
book_words %>%
filter(book == "Pride & Prejudice") %>%
select(-total) %>%
arrange(desc(tf_idf))
library(dplyr)
library(tidytext)
library(janeaustenr)
library(stringr)
library(magrittr)
library(tokenizers)
library("tm")
filePath <- "~/Dropbox/R/TidyTextMining/1Peter"
text <- readLines(filePath)
tokenize_sentences(text)
docs %>% unnest_tokens(word, text)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
docs
docs %>% unnest_tokens(word, text)
docs %>% unnest_tokens(word, text)
docs
docs %>% unnest_tokens()
docs$`1`
filePath <- "~/Dropbox/R/TidyTextMining/1Peter"
text <- readLines(filePath)
docs <- tokenize_sentences(text)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
library(dplyr)
library(tidytext)
library(janeaustenr)
library(stringr)
library(magrittr)
library(tokenizers)
library("tm")
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs
docs
attributes(docs)
attributes(docs)
docs
docs
docs %>% unnest_tokens()
docs
docs[1]
docs[[1]]
docs[[1]][1]
data("crude")
crude[[1]]
(f <- content_transformer(function(x, pattern) gsub(pattern, "", x)))
tm_map(crude, f, "[[:digit:]]+")[[1]]
crude[[1]]
crude[[1]][1]
data("crude")
crude[[1]][1]
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
tm_map(crude, f, "[[:digit:]]+")[[1]]
data("crude")
crude[[1]][1]
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
crude2 <- tm_map(crude, f, "[[:digit:]]+")[[1]][1]
data("crude")
crude[[1]][1]
f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
crude2 <- tm_map(crude, f, "[[:digit:]]+")[[1]][1]
crude2
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
attributes(docs)
attr(docs)
colnames(docs)
colnames(docs)
docs[[1]][1]
class(docs)
docs[[1]][0]
colnames(docs)
docs[[1]][1]
colnames(docs)
docs[[1]][2]
colnames(docs)
docs[[0]][2]
docs[[1]][2]
colnames(docs)
as.data.frame(docs)
library(gutenbergr)
dickensBooks <- gutenberg_download(c(675,25985, 917 676, 1023, 37121, 42232, 699, 653,20795,766,1415,821,810,1422,1400,786,644,98,580,730,883,809,1394,2324,807,927,888,963,15618,23344,968,588,1407,700,1423,967,564,27924,912))
dickensBooks <- gutenberg_download(c(675,25985, 917,676, 1023, 37121, 42232, 699, 653,20795,766,1415,821,810,1422,1400,786,644,98,580,730,883,809,1394,2324,807,927,888,963,15618,23344,968,588,1407,700,1423,967,564,27924,912))
dickensBooks <- gutenberg_download(c(675,25985,917,676,1023,37121,42232,699,653,20795,766,1415,821,810,1422,1400,786,644,98,580,730,883,809,1394,2324,807,927,888,963,15618,23344,968,588,1407,700,1423,967,564,27924,912))
tidy_dickens <- dickensBooks %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
library(gutenbergr)
library(tidytext)
tidy_dickens <- dickensBooks %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
tidy_dickens <- dickensBooks %>% unnest_tokens(word, text) %>% anti_join(stop_words)
library(gutenbergr)
library(tidytext)
tidy_dickens <- dickensBooks %>% unnest_tokens(word, text) %>% anti_join(stop_words)
library(gutenbergr)
library(tidytext)
library(dplyr)
tidy_dickens <- dickensBooks %>% unnest_tokens(word, text) %>% anti_join(stop_words)
tidy_dickens %>%
count(word, sort = TRUE)
gutenberg_works(author = "Dickens, Charles")
gutenberg_works(author == "Dickens, Charles")
charles <- gutenberg_works(author == "Dickens, Charles")
charles
View(charles)
tidy_dickens %>%
count(word, sort = TRUE) %>%
filter(n > 1000) %>%
mutate(word = reorder(work, n)) %>%
ggplot(aes(word,n)) +
geom_Bar(stat = "identity") +
xlab(NULL) +
coord_flip()
tidy_dickens %>%
count(word, sort = TRUE) %>%
filter(n > 1000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word,n)) +
geom_Bar(stat = "identity") +
xlab(NULL) +
coord_flip()
library(gutenbergr)
library(tidytext)
library(dplyr)
library(ggplot2)
tidy_dickens %>%
count(word, sort = TRUE) %>%
filter(n > 1000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word,n)) +
geom_Bar(stat = "identity") +
xlab(NULL) +
coord_flip()
tidy_dickens %>%
count(word, sort = TRUE) %>%
filter(n > 1000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word,n)) +
geom_bar(stat = "identity") +
xlab(NULL) +
coord_flip()
tidy_dickens %>%
count(word, sort = TRUE) %>%
filter(n > 2000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word,n)) +
geom_bar(stat = "identity") +
xlab(NULL) +
coord_flip()
tidy_dickens %>%
count(word, sort = TRUE) %>%
filter(n > 3000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word,n)) +
geom_bar(stat = "identity") +
xlab(NULL) +
coord_flip()
tidy_dickens %>%
count(word, sort = TRUE) %>%
filter(n > 3000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word,n)) +
geom_bar(stat = "identity") +
xlab(NULL) +
coord_flip() +
title("3000 Word occurences or more")
tidy_dickens %>%
count(word, sort = TRUE) %>%
filter(n > 3000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word,n)) +
geom_bar(stat = "identity") +
xlab(NULL) +
coord_flip() +
title(c("3000 Word occurences or more"))
tidy_dickens %>%
count(word, sort = TRUE) %>%
filter(n > 3000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word,n)) +
geom_bar(stat = "identity") +
xlab(NULL) +
coord_flip() +
ggtitle("3000 Word occurences or more")
save(dickensBooks, file="dickensBooks.RData")
file.exists("dickensBooks.RData")
library(gutenbergr)
library(tidytext)
library(dplyr)
library(ggplot2)
if(file.exists("dickensBooks.RData"))
{
dickensBooks <-  load("dickensBooks.RData")
}
if(!file.exists("dickensBooks.RData")){
dickensBooks <- gutenberg_download(c(675,25985,917,676,1023,37121,42232,699,653,20795,766,1415,821,810,1422,1400,786,644,98,580,730,883,809,1394,2324,807,927,888,963,15618,23344,968,588,1407,700,1423,967,564,27924,912),meta_fields = c("title","author","language")
)
save(dickensBooks, file="dickensBooks.RData")
}
load("~/Dropbox/R/TidyTextMining/dickensBooks.RData")
tidy_dickens <- dickensBooks %>% unnest_tokens(word, text) %>% anti_join(stop_words)
tidy_dickens
dickensBooks<- tokenize_sentences(dickensBooks$text)
library(tokenizers)
dickensBooks<- tokenize_sentences(dickensBooks$text)
dickensBooks<- tokenize_sentences(stri_enc(toutf8(dickensBooks$text))
)
dickensBooks3 <- stri_enc_toutf8(dickensBooks$text)
dickensBooks3 <- stri_enc_toutf8(dickensBooks$text)
dickensBooks3 <- stri_enc_toutf8(dickensBooks$text)
library(stringi)
dickensBooks3 <- stri_enc_toutf8(dickensBooks$text)
dickensBooks3
tidy_dickens <- dickensBooks %>% unnest_tokens(word, text, token = "sentences") %>% anti_join(stop_words)
tidy_dickens <- dickensBooks %>% unnest_tokens(word, text, token = "sentences") %>% anti_join(stop_words)
dickensBooks$text <- stri_enc_toutf8(dickensbooks$text)
dickensBooks$text <- stri_enc_toutf8(dickensBooks$text)
tidy_dickens <- dickensBooks %>% unnest_tokens(word, text, token = "sentences") %>% anti_join(stop_words)
dickensBooks$text <- stri_enc_toutf8(dickensBooks$text,is_unknown_8bit = TRUE )
tidy_dickens <- dickensBooks %>% unnest_tokens(word, text, token = "sentences") %>% anti_join(stop_words)
dickensBooks$text <- stri_enc_toutf8(dickensBooks$text,is_unknown_8bit = TRUE, validate = TRUE )
tidy_dickens <- dickensBooks %>% unnest_tokens(word, text, token = "sentences") %>% anti_join(stop_words)
tidy_dickens
dickensBooks$text <- stri_enc_toutf8(dickensBooks$text,is_unknown_8bit = TRUE, validate = TRUE )
tidy_dickens <- dickensBooks %>% unnest_tokens(word, text, token = "sentences") %>% unnest_tokens(word, text) anti_join(stop_words)
dickensBooks$text <- stri_enc_toutf8(dickensBooks$text,is_unknown_8bit = TRUE, validate = TRUE )
tidy_dickens <- dickensBooks %>% unnest_tokens(word, text, token = "sentences")
%>% unnest_tokens(word, text) anti_join(stop_words)
dickensBooks$text <- stri_enc_toutf8(dickensBooks$text,is_unknown_8bit = TRUE, validate = TRUE )
tidy_dickens <- dickensBooks %>% unnest_tokens(word, text, token = "sentences") %>% unnest_tokens(word, text) anti_join(stop_words)
dickensBooks$text <- stri_enc_toutf8(dickensBooks$text,is_unknown_8bit = TRUE, validate = TRUE )
tidy_dickens <- dickensBooks %>% unnest_tokens(word, text, token = "sentences") %>% unnest_tokens(word, text) %>% anti_join(stop_words)
dickensBooks$text <- stri_enc_toutf8(dickensBooks$text,is_unknown_8bit = TRUE, validate = TRUE )
tidy_dickens <- dickensBooks %>% unnest_tokens(word, text, token = "sentences") %>% anti_join(stop_words)
tidy_dickens
dickensBooks$text <- stri_enc_toutf8(dickensBooks$text,is_unknown_8bit = TRUE, validate = TRUE )
tidy_dickens <- dickensBooks %>% unnest_tokens(word, text) %>% anti_join(stop_words)
tidy_dickens
library(gutenbergr)
library(tidytext)
library(dplyr)
library(ggplot2)
library(gutenbergr)
library(tidytext)
library(dplyr)
library(ggplot2)
if(file.exists("dickensBooks.RData"))
{
dickensBooks <-  load("dickensBooks.RData")
}
if(!file.exists("dickensBooks.RData")){
dickensBooks <- gutenberg_download(c(675,25985,917,676,1023,37121,42232,699,653,20795,766,1415,821,810,1422,1400,786,644,98,580,730,883,809,1394,2324,807,927,888,963,15618,23344,968,588,1407,700,1423,967,564,27924,912),meta_fields = c("title","author","language")
)
save(dickensBooks, file="dickensBooks.RData")
}
charles <- gutenberg_works(author == "Dickens, Charles")
dickensBooks <-  load("dickensBooks.RData")
dickensBooks
dickensBooks$text <- stri_enc_toutf8(dickensBooks$text,is_unknown_8bit = TRUE, validate = TRUE )
tidy_dickens %>%
count(word, sort = TRUE) %>%
filter(n > 3000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word,n)) +
geom_bar(stat = "identity") +
xlab(NULL) +
coord_flip() +
ggtitle("Charles Dickens: 3000 Word occurences or more")
library(scales)
ggplot(frequency, aes(x = other, y = austen, color = abs(austen - other))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~author, ncol = 2) +
theme(legend.position="none") +
labs(y = "Jane Austen", x = NULL)
dickensBooks$text <- stri_enc_toutf8(dickensBooks$text,is_unknown_8bit = TRUE, validate = TRUE )
library(gutenbergr)
library(tidytext)
library(dplyr)
library(ggplot2)
library(stringi)
dickensBooks$text <- stri_enc_toutf8(dickensBooks$text,is_unknown_8bit = TRUE, validate = TRUE )
dickensBooks <-  load("dickensBooks.RData")
dickensBooks
dickensBooks$text <- stri_enc_toutf8(dickensBooks$text,is_unknown_8bit = TRUE, validate = TRUE )
tidy_dickens %>%
count(word, sort = TRUE) %>%
filter(n > 3000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word,n)) +
geom_bar(stat = "identity") +
xlab(NULL) +
coord_flip() +
ggtitle("Charles Dickens: 3000 Word occurences or more")
charles <- gutenberg_works(author == "Dickens, Charles")
View(charles)
dickensBooks$text <- stri_enc_toutf8(dickensBooks$text,is_unknown_8bit = TRUE, validate = TRUE )
dickensBooks <-  load("dickensBooks.RData")
dickensBooks
tidy_dickens <- dickensBooks %>% unnest_tokens(word, text) %>% anti_join(stop_words)
library(stringi)
library(gutenbergr)
library(tidytext)
library(dplyr)
library(ggplot2)
library(stringi)
dickensBooks$text <- stri_enc_toutf8(dickensBooks$text,is_unknown_8bit = TRUE, validate = TRUE )
dickensBooks$text <- stri_enc_toutf8(dickensBooks,is_unknown_8bit = TRUE, validate = TRUE )
tidy_dickens <- dickensBooks %>% unnest_tokens(word, text) %>% anti_join(stop_words)
dickensBooks$text <- stri_enc_toutf8(dickensBooks$text,is_unknown_8bit = TRUE, validate = TRUE )
tidy_dickens <- dickensBooks %>% unnest_tokens(word, text) %>% anti_join(stop_words)
tidy_dickens
dickensBooks$text <- stri_enc_toutf8(dickensBooks$text,is_unknown_8bit = TRUE, validate = TRUE )
tidy_dickens <- dickensBooks %>% unnest_tokens(word, text) %>% anti_join(stop_words)
