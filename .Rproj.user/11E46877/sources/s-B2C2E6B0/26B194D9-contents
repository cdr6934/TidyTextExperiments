---
title: "tidyText and the Good Charles Dickens"
output: html_notebook
---

https://github.com/ropensci/tokenizers/blob/master/vignettes/introduction-to-tokenizers.Rmd
https://briatte.github.io/ggnet/
http://rstats-db.github.io/DBI/


```{r include=FALSE}
library(gutenbergr)
library(tidytext)
library(dplyr)
library(ggplot2)
library(stringi)
```


You can do this the hard way and go to the site and collect the books you want or... 
```{r}
charles <- gutenberg_works(author == "Dickens, Charles")

if(file.exists("dickensBooks.RData"))
{
 dickensBooks <-  load("dickensBooks.RData")
}
if(!file.exists("dickensBooks.RData")){
  dickensBooks <- gutenberg_download(charles$gutenberg_id,meta_fields = c("title","author","language")
)
  save(dickensBooks, file="dickensBooks.RData")
}

```


```{r}
 dickensBooks <-  load("dickensBooks.RData")
```


Now what do you want to do with the books; generally we want to pull stop words within the corpus 
```{r}
dickensBooks$text <- stri_enc_toutf8(dickensBooks$text,is_unknown_8bit = TRUE, validate = TRUE )
tidy_dickens <- dickensBooks %>% unnest_tokens(word, text) %>% anti_join(stop_words)
tidy_dickens
```
You can also break it out more so that you have a better understanding of the placement of the word within the book. 



### Greater Than 3000 instances of words 
```{r}
tidy_dickens %>%
  count(word, sort = TRUE) %>%
  filter(n > 3000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word,n)) + 
  geom_bar(stat = "identity") + 
  xlab(NULL) +
  coord_flip() + 
  ggtitle("Charles Dickens: 3000 Word occurences or more")
```



